{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8uy7m9hhZ6ZppUivi68Rr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLesmes/cvSrc/blob/main/modern_cv_gen_ai/train_object_detection_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# env"
      ],
      "metadata": {
        "id": "pbPfKxoZ8-Jd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-om0GOdwmPNB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8c92eb-c933-40a2-945d-474f8824a583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-cv==0.6.1\n",
            "  Downloading keras_cv-0.6.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting keras-core\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-cv==0.6.1) (24.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-cv==0.6.1) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-cv==0.6.1) (2024.9.11)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras-cv==0.6.1) (4.9.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-core) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-core) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core) (3.12.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core) (0.1.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core) (4.12.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (8.1.7)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (4.2.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (17.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (2.32.3)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (1.16.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (2.5.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (4.66.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (1.16.0)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv==0.6.1) (0.5.1)\n",
            "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv==0.6.1) (1.10.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv==0.6.1) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv==0.6.1) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv==0.6.1) (3.20.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv==0.6.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv==0.6.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv==0.6.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv==0.6.1) (2024.8.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets->keras-cv==0.6.1) (1.16.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->keras-cv==0.6.1) (0.16)\n",
            "Downloading keras_cv-0.6.1-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.2/756.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-core, keras-cv\n",
            "Successfully installed keras-core-0.1.7 keras-cv-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-cv==0.6.1 keras-core"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow==2.15.1\n",
        "!pip install -q tensorflow_hub==0.12.0\n",
        "!pip install -q tensorflow_text==2.15.0\n",
        "!pip install -q top2vec[sentence_transformers]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZo4F9c5C8kR",
        "outputId": "d87079b7-1dd6-4038-cb2a-279dd5866621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "071iZFKMDgzy",
        "outputId": "858a3f16-1c9f-4922-ab71-2c97cfe332b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keras==2.15.0\n",
            "keras-core==0.1.7\n",
            "keras-cv==0.6.1\n",
            "tf_keras==2.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DVz_JOSDzKS",
        "outputId": "72b37673-496d-430e-94f1-1a80952994f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow==2.15.1\n",
            "tensorflow-datasets==4.9.6\n",
            "tensorflow-estimator==2.15.0\n",
            "tensorflow-hub==0.16.1\n",
            "tensorflow-io-gcs-filesystem==0.37.1\n",
            "tensorflow-metadata==1.16.1\n",
            "tensorflow-probability==0.24.0\n",
            "tensorflow-text==2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "Mml8Y-d2Axc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "#import tensorflow.keras as keras\n",
        "from tensorflow.keras import optimizers\n",
        "import keras_cv\n",
        "import numpy as np\n",
        "from keras_cv import bounding_box\n",
        "import os\n",
        "import resource\n",
        "from keras_cv import visualization\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "v_Oth1T5tJVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c2c456-853f-4252-ad78-86724c6450d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labels"
      ],
      "metadata": {
        "id": "GZRtLS-69Ga-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a dictionary pointing from int classes to class names\n",
        "class_ids = [\n",
        "    \"Aeroplane\",\n",
        "    \"Bicycle\",\n",
        "    \"Bird\",\n",
        "    \"Boat\",\n",
        "    \"Bottle\",\n",
        "    \"Bus\",\n",
        "    \"Car\",\n",
        "    \"Cat\",\n",
        "    \"Chair\",\n",
        "    \"Cow\",\n",
        "    \"Dining Table\",\n",
        "    \"Dog\",\n",
        "    \"Horse\",\n",
        "    \"Motorbike\",\n",
        "    \"Person\",\n",
        "    \"Potted Plant\",\n",
        "    \"Sheep\",\n",
        "    \"Sofa\",\n",
        "    \"Train\",\n",
        "    \"Tvmonitor\",\n",
        "    \"Total\",\n",
        "]\n",
        "\n",
        "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
        "class_mapping"
      ],
      "metadata": {
        "id": "in2shdn9tJk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7015f424-9b39-4c7d-c450-16627568803d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Aeroplane',\n",
              " 1: 'Bicycle',\n",
              " 2: 'Bird',\n",
              " 3: 'Boat',\n",
              " 4: 'Bottle',\n",
              " 5: 'Bus',\n",
              " 6: 'Car',\n",
              " 7: 'Cat',\n",
              " 8: 'Chair',\n",
              " 9: 'Cow',\n",
              " 10: 'Dining Table',\n",
              " 11: 'Dog',\n",
              " 12: 'Horse',\n",
              " 13: 'Motorbike',\n",
              " 14: 'Person',\n",
              " 15: 'Potted Plant',\n",
              " 16: 'Sheep',\n",
              " 17: 'Sofa',\n",
              " 18: 'Train',\n",
              " 19: 'Tvmonitor',\n",
              " 20: 'Total'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "zoSZvVCYtJvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize images amd bunding boxes"
      ],
      "metadata": {
        "id": "FyUOJpHg9TUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
        "  \"\"\"\n",
        "  Visualizes a dataset of images and their corresponding bounding boxes in a grid format.\n",
        "\n",
        "  Args:\n",
        "      inputs (tf.data.Dataset or similar): A dataset or iterable containing batches of images and bounding boxes.\n",
        "      value_range (tuple): The range of values for the images, typically in the form (min_value, max_value).\n",
        "      rows (int): The number of rows in the grid visualization.\n",
        "      cols (int): The number of columns in the grid visualization.\n",
        "      bounding_box_format (str): The format of the bounding boxes (e.g., \"xywh\", \"corners\", etc.). This specifies how\n",
        "          the bounding boxes are encoded.\n",
        "\n",
        "  Returns:\n",
        "      None: This function does not return anything. It displays a grid of images with bounding boxes overlaid.\n",
        "\n",
        "  Example:\n",
        "      >>> visualize_dataset(dataset, value_range=(0, 1), rows=2, cols=3, bounding_box_format='xywh')\n",
        "\n",
        "  Notes:\n",
        "      - This function assumes that `inputs` is a dataset with the structure `{\"images\": ..., \"bounding_boxes\": ...}`.\n",
        "      - `class_mapping` should be defined globally or accessible in the scope where this function is called.\n",
        "  \"\"\"\n",
        "  inputs = next(iter(inputs.take(1)))\n",
        "  images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
        "  visualization.plot_bounding_box_gallery(\n",
        "      images,\n",
        "      value_range=value_range,\n",
        "      rows=rows,\n",
        "      cols=cols,\n",
        "      y_true=bounding_boxes,\n",
        "      scale=5,\n",
        "      font_scale=0.7,\n",
        "      bounding_box_format=bounding_box_format,\n",
        "      class_mapping=class_mapping,\n",
        "  )"
      ],
      "metadata": {
        "id": "wXrpRY_ltblO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bounding box formats mapping"
      ],
      "metadata": {
        "id": "N28pZ6DA9hAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To unpackage the tensorflow input we will use the [rel_xyxy](https://keras.io/api/keras_cv/bounding_box/formats/#rel_xyxy-class) format ✅"
      ],
      "metadata": {
        "id": "p9tNY4GBGDlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unpackage_raw_tfds_inputs(inputs, bounding_box_format):\n",
        "  \"\"\"\n",
        "    Unpacks and processes raw inputs from a TensorFlow Datasets (TFDS) dataset, converting bounding boxes to the desired format.\n",
        "\n",
        "    Args:\n",
        "        inputs (dict): A dictionary containing raw data from a TFDS dataset, with keys:\n",
        "            - \"image\": The input image.\n",
        "            - \"objects\": A dictionary containing bounding box information with keys:\n",
        "                - \"bboxes\": The bounding boxes in the source format (e.g., \"rel_xyxy\").\n",
        "                - \"label\": The labels or class ids for the objects.\n",
        "        bounding_box_format (str): The target format for the bounding boxes (e.g., \"xywh\", \"corners\", etc.). This specifies how\n",
        "            the bounding boxes should be encoded for further processing.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the following structure:\n",
        "            - \"images\": The image tensor, cast to `tf.float32`.\n",
        "            - \"bounding_boxes\": A dictionary containing:\n",
        "                - \"classes\": The class labels, cast to `tf.float32`.\n",
        "                - \"boxes\": The bounding boxes, cast to `tf.float32`, and converted to the specified target format.\n",
        "\n",
        "    Example:\n",
        "        >>> unpackage_raw_tfds_inputs(inputs, bounding_box_format=\"xywh\")\n",
        "\n",
        "    Notes:\n",
        "        - The source format for the bounding boxes is assumed to be \"rel_xyxy\" (relative coordinates in the format [x_min, y_min, x_max, y_max]).\n",
        "        - The `keras_cv.bounding_box.convert_format` utility is used to convert bounding boxes from the source to the target format.\n",
        "  \"\"\"\n",
        "  image = inputs[\"image\"]\n",
        "  boxes = keras_cv.bounding_box.convert_format(\n",
        "    inputs[\"objects\"][\"bboxes\"],\n",
        "    images=image,\n",
        "    source=\"rel_xyxy\",\n",
        "    target=bounding_box_format,\n",
        "  )\n",
        "\n",
        "  bounding_boxes = {\n",
        "    \"classes\": tf.cast(inputs[\"objects\"][\"label\"], dtype=tf.float32),\n",
        "    \"boxes\": tf.cast(boxes, dtype=tf.float32),\n",
        "  }\n",
        "\n",
        "  return {\n",
        "    \"images\": tf.cast(image, tf.float32),\n",
        "    \"bounding_boxes\": bounding_boxes,\n",
        "  }"
      ],
      "metadata": {
        "id": "iYvdgiPmtcEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pascal_voc(split, dataset, bounding_box_format):\n",
        "  \"\"\"\n",
        "    Loads and preprocesses the PASCAL VOC dataset using TensorFlow Datasets (TFDS).\n",
        "\n",
        "    Args:\n",
        "        split (str): The subset of the dataset to load (e.g., 'train', 'validation', or 'test').\n",
        "        dataset (str): The name of the dataset to load (e.g., 'voc/2012').\n",
        "        bounding_box_format (str): The format of the bounding boxes (e.g., 'xywh', 'corners').\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: A TensorFlow dataset containing the preprocessed images and bounding boxes.\n",
        "  \"\"\"\n",
        "  ds = tfds.load(\n",
        "      dataset,\n",
        "      split=split,\n",
        "      with_info=False,\n",
        "      shuffle_files=True\n",
        "    )\n",
        "  ds = ds.map(\n",
        "      lambda x: unpackage_raw_tfds_inputs(\n",
        "          x, bounding_box_format=bounding_box_format,\n",
        "          num_parallel_calls=tf.data.AUTOTUNE,\n",
        "      )\n",
        "  )\n",
        "  return ds"
      ],
      "metadata": {
        "id": "pEwXwNsctc4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "ilSoctMZ7qom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading and formatting the data"
      ],
      "metadata": {
        "id": "eWA9FAlF8MPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_ds = load_pascal_voc(\n",
        "    split=\"train\",\n",
        "    dataset=\"voc/2007\",\n",
        "    bounding_box_format=\"xywh\"\n",
        ")\n",
        "\n",
        "eval_ds = load_pascal_voc(\n",
        "    split=\"test\",\n",
        "    dataset=\"voc/2007\",\n",
        "    bounding_box_format=\"xywh\"\n",
        ")\n",
        "\n",
        "train_ds = train_ds.shuffle(BATCH_SIZE * 4)"
      ],
      "metadata": {
        "id": "82CGOd2_tdGS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "9c5f1710-dbe1-4f11-d74d-c8adb31884e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"<ipython-input-9-22c47632f68c>\", line 20, in None  *\n        )\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__unpackage_raw_tfds_inputs() got an unexpected keyword argument 'num_parallel_calls'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-22c47632f68c>\u001b[0m in \u001b[0;36mload_pascal_voc\u001b[0;34m(split, dataset, bounding_box_format)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mshuffle_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[0;32m---> 19\u001b[0;31m   ds = ds.map(\n\u001b[0m\u001b[1;32m     20\u001b[0m       lambda x: unpackage_raw_tfds_inputs(\n\u001b[1;32m     21\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounding_box_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounding_box_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2278\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2279\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2280\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2281\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     35\u001b[0m       warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m     36\u001b[0m                     \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m---> 37\u001b[0;31m     return _MapDataset(\n\u001b[0m\u001b[1;32m     38\u001b[0m         input_dataset, map_func, preserve_cardinality=True, name=name)\n\u001b[1;32m     39\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# Note: wrapper_helper will apply autograph based on context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filetny06jaj.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtf__lam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_function_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpackage_raw_tfds_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounding_box_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounding_box_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lscope'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__lam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/core/function_wrappers.py\u001b[0m in \u001b[0;36mwith_function_scope\u001b[0;34m(thunk, scope_name, options)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;34m\"\"\"Inline version of the FunctionScope context manager.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mFunctionScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lambda_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/__autograph_generated_filetny06jaj.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(lscope)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtf__lam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_function_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpackage_raw_tfds_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounding_box_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounding_box_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lscope'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__lam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"<ipython-input-9-22c47632f68c>\", line 20, in None  *\n        )\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__unpackage_raw_tfds_inputs() got an unexpected keyword argument 'num_parallel_calls'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use ragged batch since images can be of different sizes and each image can have variable number of objects"
      ],
      "metadata": {
        "id": "wyEn3-wR8Udn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.ragged_batch(\n",
        "    BATCH_SIZE,\n",
        "    drop_remainder=True\n",
        "  )\n",
        "eval_ds = eval_ds.ragged_batch(\n",
        "    BATCH_SIZE,\n",
        "    drop_remainder=True\n",
        "  )"
      ],
      "metadata": {
        "id": "_Xn_wuyCtdQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the dataset to ensure bounding boxes are in the right place\n",
        "with correct labels. If done incorrectly, bounding boxes will not appear\n",
        "or they will be in the wrong place."
      ],
      "metadata": {
        "id": "3HC4MvQa8dqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_dataset(\n",
        "    train_ds,\n",
        "    bounding_box_format=\"xywh\",\n",
        "    value_range=(0, 255),\n",
        "    rows=2,\n",
        "    cols=2\n",
        ")"
      ],
      "metadata": {
        "id": "7J18i65w8bSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize validation set"
      ],
      "metadata": {
        "id": "wTJzlvDP8s3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_dataset(\n",
        "    eval_ds,\n",
        "    bounding_box_format=\"xywh\",\n",
        "    value_range=(0, 255),\n",
        "    rows=2,\n",
        "    cols=2,\n",
        ")"
      ],
      "metadata": {
        "id": "Gu789Ouutdqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation\n",
        "Data augmentation is complex since after the image is modified, the bounding boxes must also be modified accordingly!"
      ],
      "metadata": {
        "id": "ELCa9ym98y5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmenter = keras.Sequential(\n",
        "    layers=[\n",
        "        keras_cv.layers.RandomFlip(\n",
        "            mode=\"horizontal\",\n",
        "            bounding_box_format=\"xywh\",\n",
        "        ),\n",
        "        keras_cv.layers.JitteredResize(\n",
        "            target_size=(640, 640),\n",
        "            scale_factor=(0.75, 1.3),\n",
        "            bounding_box_format=\"xywh\",\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "eUGmpRDRtd93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_ds = train_ds.map(\n",
        "    augmenter,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "visualize_dataset(\n",
        "    train_ds,\n",
        "    bounding_box_format=\"xywh\",\n",
        "    value_range=(0, 255),\n",
        "    rows=2,\n",
        "    cols=2\n",
        ")"
      ],
      "metadata": {
        "id": "3VcSkk9UteLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use deterministic resizing for the validation set"
      ],
      "metadata": {
        "id": "fr83e-7Y9JYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "inference_resizing = keras_cv.layers.Resizing(\n",
        "    640,\n",
        "    640,\n",
        "    bounding_box_format=\"xywh\",\n",
        "    pad_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "eval_ds = eval_ds.map(\n",
        "    inference_resizing,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        "  )\n"
      ],
      "metadata": {
        "id": "ElIZkoQExPtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make sure the resizing worked\n"
      ],
      "metadata": {
        "id": "D2ttJLH79QsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_dataset(\n",
        "    eval_ds,\n",
        "    bounding_box_format=\"xywh\",\n",
        "    value_range=(0, 255),\n",
        "    rows=2,\n",
        "    cols=2\n",
        ")"
      ],
      "metadata": {
        "id": "a3yw2VqZ9Tbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input format\n",
        "\n",
        "This is the final form our model expects:\n",
        "* tuple of (images, bounding_box_dictionary)\n",
        "* to_dense() makes the batch compatible with TPU\n"
      ],
      "metadata": {
        "id": "tQyav3Kd9Z4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_to_tuple(inputs):\n",
        "    return inputs[\"images\"], bounding_box.to_dense(\n",
        "        inputs[\"bounding_boxes\"],\n",
        "        max_boxes=32\n",
        "    )\n",
        "\n",
        "train_ds = train_ds.map(\n",
        "    dict_to_tuple,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "eval_ds = eval_ds.map(\n",
        "    dict_to_tuple,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "eval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "UU9UeZB3xRON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hiperparameters\n",
        "\n",
        "> Including a global_clipnorm is extremely important in object detection tasks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kDloZ99W98WX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_lr = 0.005\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(\n",
        "    learning_rate=base_lr,\n",
        "    momentum=0.9,\n",
        "    global_clipnorm=10.0\n",
        ")"
      ],
      "metadata": {
        "id": "10bRL7AwxTLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture\n",
        "Creates a \"RetinaNet\" from ResNet50 backbone"
      ],
      "metadata": {
        "id": "XhCwDsbO-RYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras_cv.models.RetinaNet.from_preset(\n",
        "    \"resnet50_imagenet\",\n",
        "    num_classes=len(class_mapping),\n",
        "    bounding_box_format=\"xywh\",\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    classification_loss=\"focal\",\n",
        "    box_loss=\"smoothl1\",\n",
        "    optimizer=optimizer,\n",
        ")"
      ],
      "metadata": {
        "id": "Ef2UgoVk-Pou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "> *Remove take(20) for full training (takes very long!)*\n",
        "\n"
      ],
      "metadata": {
        "id": "omzGtbSS-rfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model.fit(\n",
        "    train_ds.take(20),\n",
        "    validation_data=eval_ds.take(20),\n",
        "    epochs=10,\n",
        ")"
      ],
      "metadata": {
        "id": "3hBpOUjixWPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load a fully trained model to test predictions"
      ],
      "metadata": {
        "id": "p5hbZeJD-6PI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras_cv.models.RetinaNet.from_preset(\n",
        "    \"retinanet_resnet50_pascalvoc\",\n",
        "    bounding_box_format=\"xywh\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "tFWHE9oO-3eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "construct a dataset with larger batches:"
      ],
      "metadata": {
        "id": "XdIL8x3F_A_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%% time\n",
        "visualization_ds = eval_ds.unbatch()\n",
        "visualization_ds = visualization_ds.ragged_batch(16)\n",
        "visualization_ds = visualization_ds.shuffle(8)"
      ],
      "metadata": {
        "id": "5gdH8Zt0--C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "P6xIW90y_q_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_detections(model, dataset, bounding_box_format):\n",
        "  \"\"\"\n",
        "    Visualizes object detection results by showing predicted bounding boxes\n",
        "    alongside ground truth boxes for a batch of images.\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): The trained object detection model used to make predictions.\n",
        "        dataset (tf.data.Dataset): The dataset from which to sample a batch of images. It should\n",
        "            yield images and their corresponding ground truth bounding boxes.\n",
        "        bounding_box_format (str): The format of the bounding boxes (e.g., 'xywh', 'corners').\n",
        "            This format must be consistent with both the ground truth boxes and the predicted boxes.\n",
        "\n",
        "    Returns:\n",
        "        None: The function creates a gallery of images with bounding boxes and displays it.\n",
        "\n",
        "    Visualization Parameters:\n",
        "        - The gallery will show both ground truth (y_true) and predicted (y_pred) bounding boxes.\n",
        "        - The layout of the gallery is 2 rows and 4 columns, showing a total of 8 images.\n",
        "        - Bounding boxes will be displayed with class labels, and the font size for annotations is adjustable.\n",
        "  \"\"\"\n",
        "  images, y_true = next(iter(dataset.take(1)))\n",
        "  y_pred = model.predict(images)\n",
        "  y_pred = bounding_box.to_ragged(y_pred)\n",
        "  visualization.plot_bounding_box_gallery(\n",
        "      images,\n",
        "      value_range=(0, 255),\n",
        "      bounding_box_format=bounding_box_format,\n",
        "      y_true=y_true,\n",
        "      y_pred=y_pred,\n",
        "      scale=4,\n",
        "      rows=2,\n",
        "      cols=4,\n",
        "      show=True,\n",
        "      font_scale=0.7,\n",
        "      class_mapping=class_mapping,\n",
        "  )"
      ],
      "metadata": {
        "id": "KI3lx19Dx3qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ***To be clear, this model already has a prediction decoder! This is just showing you how to set parameters yourself.***\n",
        "\n",
        "Set IoU and confidence threshold\n"
      ],
      "metadata": {
        "id": "I3WWtHGMx9It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "model.prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(\n",
        "    bounding_box_format=\"xywh\",\n",
        "    from_logits=True,\n",
        "    iou_threshold=0.5,\n",
        "    confidence_threshold=0.75,\n",
        ")\n",
        "\n",
        "visualize_detections(\n",
        "  model,\n",
        "  dataset=visualization_ds,\n",
        "  bounding_box_format=\"xywh\"\n",
        ")"
      ],
      "metadata": {
        "id": "5IPLGvHTx5GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ypYfDfyCyDPp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}